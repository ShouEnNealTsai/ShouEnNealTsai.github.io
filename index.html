<!DOCTYPE HTML>
<html>
	<head>
		<title>Shou En Tsai</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
			<header id="header">
				<!--
				<div class="logo">
					<span class="icon fa-code"></span>
				</div>
			-->
				<div class="content">
					<div class="inner">
						<h1>Shou-En Tsai</h1>
						<p>HCI Researcher
							<br/>
							Tangible Interface on body
							<br/>
							Human Computer Integration(HInt)
							<br/>
							Actively Applying for HCI Ph.D.
						</p>

					</div>
				</div>
				<nav>
					<ul>
						<li><a href="#about">About</a></li>
						<li><a href="#exp">Research</a></li>
						<li><a href="#work">Projects</a></li>
						<li><a href="#misc">Misc</a></li>
					</ul>
				</nav>
			</header>

			<!-- Main -->
			<div id="main">

				<!-- About -->
				<article style="width: 55rem;" id="about">
					<h2 class="major">About</h2>
					<span class="image bg" style="display:block; margin:auto;"><img src="images/me.jpg" alt="" /></span>
					<p>
						<br/>
						I am an HCI researcher who want to pursue a Ph.D. I am interested in tangible user interface on body and human-computer integration (HInt) devices for physical and cognitive enhancement. My research interest is to break the boundary between the human body and devices, to meld users and tech objects being used. The tangible interfaces experiences apply to coupling users and systems, a medium to carry ambiguous interfaces instantiated among humans, devices, the physical world, and cyberspace.
						<br/>
						<br/>
						I am currently working with Prof. <a href="http://mislab.cs.nthu.edu.tw/" target="_blank">Min-Chun Hu</a> in NTHU, developing a portable device about the immersive experience of olfactory feedback in VR. I'm also working with Prof. <a href="https://dcat.nctu.edu.tw/en/members/%e9%99%b3%e7%9b%88%e7%be%bd/#content" target="_blank">Ying-Yu Chen</a>, a project about collaborative and tangible VR riprap tutorial for children.
					</p>

					<h3 class="major">Education</h3>

					<div style="font-weight: bold;">National Tsing-Hua University<span style="font-weight: normal;">, Hsinchu, Taiwan</span><span style="float:right;">09/2015 - 06/2020</span></div>
					<ul style="padding-left: 5%;">
						<li>B.Sc. Electrical Engineering and Computer Science</li>
					</ul>


					<h3 class="major">Others</h3>
					<ul class="icons">
						<li><a href="https://twitter.com/leo12330731" target="_blank" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="mailto:leo12330731@gmail.com" class="icon fa-envelope"><span class="label">EMail</span></a></li>
					</ul>

				</article>

				<!-- Experiences -->
				<article id="exp">
					<h2 class="major">Research</h2>

					<h3 class="major">UCSD ProtoLab</h3>
					<div style="font-weight: bold;">Visiting Student, ProtoLab, UC San Diego<span style="font-weight: normal;"> - advised by Prof. <a href="http://spdow.ucsd.edu/" target="_blank">Steven Dow</a></span>
						<span style="float: right;">07/2017 - 08/2017</span></div>

						<ul style="padding-left: 5%;">
							<li>
								 Assist visiting scholar, <a href="https://scholar.google.dk/citations?user=yiqsgWcAAAAJ&hl=en" target="_blank">Nanna Inie</a> to develop the software that her research projects needed.
							</li>
							<br/>
							<br/>
						</ul>

					<h3 class="major">GREENS</h3>
					<span class="image half"><img src="images/GREENS_ONE.png" alt="" /></span>
					<span class="image half" style="margin-bottom:20px;"><img src="images/GREENS_TWO.png" alt="" /></span>
					<div style="font-weight: bold;">Research Assistant, IDD Lab, NCTU<span style="font-weight: normal;">- advised by Prof. <a href="https://twitter.com/dayuanhuang" target="_blank">Da-Yuan Huang</a></span>
						<span style="float: right;">07/2019 - 10/2019</span></div>
						<ul style="padding-left: 5%;">
							<li>
								 GREENS: Revealing invisible environment data as an auto-generated melody with human collaboration
								 <br/>
								 GREENS was a system that would auto-generated music from invisible environmental data and humans' status. GREENS used Google Dialogflow to build the voice-based interface. Users' voice inputs enable the Coral TPU board to trigger the pygame module to play the auto-generated jazz music produced by Tensorflow and pymidi module, and also would change our projecting images controlled by Madmapper.
							</li>
							<br/>
							<li>
								 GREENS won the Jury's Choice Award Honorable Mention and People's Choice Award.
							</li>
							<br/>
							<br/>
						</ul>

					<h3 class="major">Virtual Cues Influence the Olfactory Perception</h3>
					<span class="image left" style="margin-bottom:40px;"><img style=" height: 350px; object-fit: fill;" src="images/Olfactory_Display.png" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/Olfactory_Display_2.png" alt="" /></span>
					<div style="font-weight: bold;">Research Assistant, MISLab, NTHU
						<span style="font-weight: normal;"> - advised by Prof. 	<a href="http://mislab.cs.nthu.edu.tw/" target="_blank">Min-Chun Hu</a>
						</span>
						<span style="float: right;">07/2019 - Present</span>
					</div>
					<ul style="padding-left: 5%;">
						<li>I designed a portable olfactory display for VR and an experiment to investigate whether visual cues can fool or stimulate the VR smell experience, regarding intensity and directionality. The display employed an Arduino as a controller to change the different levels of olfactory characteristics. The olfactory display unit is composed of a scent generator and a scent deliverer. I put an absorbent cotton core inside a reservoir filled with essential oil to absorb the oil in the scent generator. An ultrasonic atomizer was attached to the cotton core to diffuse the odor into the air. For the scent deliverer, a 5-V fan with a two-pin connector provided the active airflow delivery. I used a MOSFET to modulate the airflow intensity by pulse-width modulation (PWM). The result showed cross-modal interactions between visually virtual odor cues in VR and real olfactory stimuli from the device.</li>
						<br/>
						<br/>
					</ul>

					<h3 class="major">Design Fiction: Dream Interface Game Console</h3>
					<span class="image left" style="margin-bottom:40px;"><img style="height: 350px; object-fit: fill;" src="images/Design_Fiction_Poster.png" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/Design_Fiction_2.jpeg" alt="" /></span>
					<div style="font-weight: bold;">Research Assistant, Everyday Interaction Lab, NCTU<span style="font-weight: normal;"> - advised by Prof. <a href="https://dcat.nctu.edu.tw/en/members/%e9%99%b3%e7%9b%88%e7%be%bd/#content" target="_blank">Ying-Yu Chen</a></span>
						<span style="float: right;">07/2020 - Present</span></div>
					<ul style="padding-left: 5%;">
						<li>I join Professor Ying-Yu Chen's weekly Research Through Design(RtD) study group. It is an informal seminar to analyze some classic and influential papers about RtD and discuss some research topic trends in the HCI community. In this summer, the study group decided to execute our own speculative design by design fiction. My fiction is about a dream interface, a game console that controls the user's brain through decoding users' brainwaves and encoding the information by changing electromagnetic field to create any lucid dream. </li>
						<br/>
						<br/>
					</ul>

					<h3 class="major">Collaborative and Tangible VR Riprap Tutorial for Children</h3>
					<span class="image left" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/riprap_field.png" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 350px; object-fit: fill;" src="images/riprap_code.png" alt="" /></span>
					<div style="font-weight: bold;">Research Assistant, Everyday Interaction Lab, NCTU<span style="font-weight: normal;"> - advised by Prof. <a href="https://dcat.nctu.edu.tw/en/members/%e9%99%b3%e7%9b%88%e7%be%bd/#content" target="_blank">Ying-Yu Chen</a></span>
						<span style="float: right;">07/2020 - Present</span></div>
					<ul style="padding-left: 5%;">
						<li>We want to preserve evanescent Taiwanese Hakka style riprap techniques due to urbanization by developing a collaborative and tangible VR riprap tutorial for children. We choose to build a VR multiplayer system since ripraps need many people to build together while the tangible way in VR, such as realizing gravity, enables children to experience intuitively while breaking the spatial limits but remaining some trueness. The pictures show our field studies and our qualitative research process. </li>
					</ul>


<!--
					<div style="font-weight: bold;">Independent Researcher<span style="font-weight: normal;">
						<span style="float: right;">08/2017 - Present</span></div>
					<ul style="padding-left: 5%;">
						<li>We want to explore and construct a system, HapticVision, to expand the backward peripersonal space with tactile feedback or other somatosensory feedback. HapticVision is a new system concept that uses a sensing device to detect backward visual stimuli. Once receiving stimuli, it will activate individuals’ wearable device to give temperature-haptic feedback to extend the Peripersonal Space boundaries.</li>
					</ul>
-->
				</article>

				<!-- Work -->
				<article id="work">
					<h2 class="major">Projects</h2>
					<!--
					<div style="font-weight: bold;">President, Quarter to Ten<span style="font-weight: normal;">
						<span style="float: right;">01/2018 - 07/2018</span></div>
					<ul style="padding-left: 5%;">
						<li>Quarter to Ten is an official student startup group in National Tsing Hua University and is a sub-organization under Tsing Hua Entrepreneur Network (TEN), an official organization to assist Tsing Hua alumni entrepreneurs and provide the needed resources and help when building their enterprise.</li>
					</ul>
					-->
					<h3 class="major">LimbCare</h3>
					<span class="image left" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/LimbCare_Hardware.png" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/LimbCare_overview.png" alt="" /></span>
					<span class="image half" style="margin-bottom:20px;"><img src="images/LimbCare_App.png" alt="" /></span>
					<div style="font-weight: bold;">The Young Entrepreneurs of the Future program<span style="font-weight: normal;">
						<span style="float: right;">01/2017 - 12/2017</span></div>
					<ul style="padding-left: 5%;">
						<li>LimbCare uses the temperature control technology to avoid all the troubles of rehabilitation preparation in advance, allowing users to do hot and cold compresses in precise temperature and period at any time and in anywhere. LimbCare also integrates information and other rehabilitation methods in the app to speed up the recovery of sports injuries. We used BLE Chip (NRF8001), which integrated a bluetooth module and System-on-a-Chip, as a main processor to control a thermoelectric cooling chip, a fan for cooling processor, and a bluetooth connection with our app.
						</li>
						<br/>
						<br/>
					</ul>


					<h3 class="major">HapticVision</h3>

					<span class="image left" style=" height: 50%;"><img style="height: 350px; object-fit: fill;" src="images/HapticVision_one.png" alt="" /></span>
					<span class="image right" style="height: 50%;"><img style="height: 350px; object-fit: fill;" src="images/HapticVision_two.png" alt="" /></span>

					<span class="image left" style="margin-bottom:30px; height: 50%;"><img style="height: 350px; object-fit: fill;" src="images/HapticVision_three.png" alt="" /></span>
					<span class="image right" style="margin-bottom:30px; height: 50%;"><img style="height: 350px; object-fit: fill;" src="images/HapticVision_four.png" alt="" /></span>
					<div>
						<span style="float: right;">12/2017 - 11/2018</span>
					</div>

					<ul style="padding-left: 5%;">
							<h4 class="major">Idea</h4>
							I noticed a phenomenon called "the eye in the back of the head," which means that people can sense objects or eye gazing behind them. This phenomenon relates to the peripersonal space (PPS). PPS is a near-body multi-sensory area. It enables humans react environmental dangers more immediately by integrating tactile, visual and auditory stimuli. PPS is plastic and its boundary changes by situations. PPS can be viewed as two parts: forward PPS and backward PPS. Backward PPS boundary is much closer to our body due to the lack of the eyes, though the sensory system in whole PPS is integrated. I wanted to explore and construct a system to expand the backward PPS with tactile feedback or other somatosensory feedback. It is my first step to chase my vision of augmented human, enhancing human cognitive ability by designing a wearable computing device as the assistance and supplement of human perceptual capabilities.
							<br/>
							<br/>
							<h4 class="major">How to Realize</h4>
							There are two steps in the experiment for me to test my designed device, first is to measure the backward PPS boundaries and the second is to validate the HapticVision system concept. In first step, to measure the backward PPS boundary, I used the auditory-tactile interaction task method invented by Canzoneri et al. (Extending peripersonal space representation without tool-use: evidence from a combined behavioral-computational approach. Frontiers in Behavioral Neuroscience 9, Article 4 (February 2015), 14 pages.) This method utilizes the concept that different sensory stimuli would be integrated in the PPS. The participants would receive the haptic stimuli feedbacks and have to push the end button on their hands as their reaction once they felt the feedbacks. During the process, there were irrelevant dynamic sounds which of those sources were approaching the subject’s back. The background dynamic sounds were to be the stimuli crossing the PPS boundary, and be the interference to determine whether the brain integrates auditory and tactile stimuli. If so, the reaction times would be significantly smaller since stimuli are inside the PPS boundary. The dynamic sounds would approach the participant at a constant speed and start at 100cm from the back of the subjects. It would end at 5cm from the participants. The dynamic sounds would randomly activate the haptic feedbacks before a concrete delay times, and each point has different delays, respectively T1 (5cm) 300ms, T2 (28.75cm) 800ms, T3 (52.5cm) 1500ms, T4 (76.25cm) 2200ms and T5 (100cm) 2700ms. (See Figure 1)
						<br/>
						<br/>
						Then, I calculated the backward PPS boundary by the participants’ reaction times. I used a sigmoidal function to describe the relationship between tactile (RTs) and timing at which haptic stimuli were delivered. The sigmoidal function was described by the following equation:
						<br/>
						<span class="image half" style="margin-bottom:5px; margin-top:5px;"><img src="images/HapticVision_function.png" alt="" /></span>
						<br/>
						where x represents the timing of cold feeling delivery in ms, y the reaction time, ymin and ymax the lower and upper saturation levels of the sigmoid, xc the value of the abscissa at the central point of the sigmoid, which is the value of x at which y = (ymin+ ymax)/2 and b establishes the slope of the sigmoid at the central point. And, xc would be the point of the backward PPS boundary.
						<br/>
						<br/>
						In second step, I set up sensing device twice the distance of the boundary radius. The device would detect the entering stimuli and then activate the wearable device and give the haptic feedbacks. Once the participants felt the haptic feedbacks, they had to push the button on their hands, and the system would record the reaction time as well. This step was the same as the first step, but the differences are that it’s twice distance and has the Actor in second step. “The Actor” was the stimuli that would approach participants from their back, and would be acted by one of my friends. The sensing device would detect whether the Actor enters the boundary, and triggered the wearable device to give haptic feedbacks. I analyzed these reaction time model if backward PPS was really happened.
						<br/>
						<br/>
						<h4 class="major">Hardware</h4>
						I used Arduino Leonardo as the central controller. Its inputs connected three modules, respectively five infrared sensors as the ranging sensors, a start button and an end button. While, its outputs connected an audio amplifier module that was realized by LM384, in which the gain was 200 and the output device was an eight-ohm speaker. The sound resource was an 8-bit, 8000Hz, wav file of wave sound. I used the PWM pin of central controller to control the output magnification of the audio amplifier. Also, I used the GPIO pin to control the output device of haptic feedbacks, containing three TEC1-04904 thermoelectric cooling chips with 5 voltages, 4 amperes. The wearable device which contains three thermoelectric cooling chips was on the participants’ right wrist and would give the cold feeling haptic feedbacks when activated. Once the participants pushed the end button, the whole system would stop counting and record the time intervals, which were the participants’ reaction times.
						<br/>
						<br/>
						<li>
						The experiment showed that HapticVision did enlarge the backward PPS twice. But the further hardware design and more complete experiment is still needed.
						</li>
						<br/>
						<br/>
					</ul>

					<h3 class="major">Oculomotor Drive</h3>
					<iframe width="600" height="400"
						src="https://www.youtube.com/embed/Sd3prVTcsJ8" style="margin-bottom:40px; display:block;
						margin:auto;">
					</iframe>
					<br/>
					<br/>
					<div style="font-weight: bold;">Microsoft Imagine Cup, Student Developer Competitions<span style="font-weight: normal;">
						<span style="float: right;">03/2018 - 04/2018</span></div>
					<ul style="padding-left: 5%;">
						<li>Oculomotor Drive is a gaze tracking mobile game to gamify eye exercise. My friends and I transferred Openface, the open source face recognization library developed by CMU Professor Mahadev Satyanarayanan's lab, from Python to Swift in order to develop iOS App more easily. And we applied on tracking gazing directions. When a player stared left, right, or at the center, the app would create a basketball throlling to the rim from the direction that the player gazed. The goal of this project is to soothe the eyeballs' muscles via the exercise of different gazing directions.</li>
					</ul>

					<h3 class="major">PhysioPunk</h3>
					<span class="image left" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/PhysioPunk_Game.PNG" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/PhysioPunk_Player.PNG" alt="" /></span>
					<div>
						<span style="float: right;">07/2020 - 09/2020</span>
					</div>
					<ul style="padding-left: 5%;">
						<h4 class="major">Idea</h4>
						PhysioPunk is a tower defense game project to explore the haziness between virtuality and reality by physiological computing. Players have to attach EMG and ECG sensors on their bodies to detect EMG and ECG signals from the left shoulder, right shoulder, low back, arms and right leg. During the game, those signals would determine the speed of enemy generation. When players are nervous, and their muscles are tight, heart rate is high, the generation speed would slow down. The minimal speed is ten-second per enemy. On the other hand, when users are relaxed due to the comfortable status of the game playing and their muscles lose, heart rate is low, the generation speed would accelerate, up to one second per enemy.
						<br/>
						<br/>
						<h4 class="major">System Implementation</h4>
						The game structure is based on open source Unity Learning Project, Tower Defense Template. I changed this open source project and connect it with Arduino to receive EMG data from Arduino. We use Arduino Uno board, AD8232 SparkFun Heart Rate Monitor and Grove EMG Detector. To detect the user's heart rate, the AD8232 SparkFun Heart Rate Monitor has three pins to connect the dry electrodes attached to RA (Right Arm), LA (Left Arm), and RL (Right Leg). Grove EMG detector gathers small muscle signals from the attached electrodes. In MAGditate Fluid, we attach three electrodes on the left shoulder, right shoulder, and low back.
						<br/>
						<br/>
						<h4 class="major">Idea Philosophy</h4>
						I envison PhysioPunk as a positive-negative feedback system, which has two states' main directions. The positive-negative feedback system is to mimic the emotional hormone feedback system, such as Adrenaline. I used an epidermal way to instantiate the result of these hormones. More importantly, I made players feel connected with this game as a whole: they are not just immersive, but their biological data is bundling with the game's status. On the flip side, the game is also affected by the players' status. They intertwine and co-influence.
						<br/>
						<br/>

					</ul>

					<!--
					PhysioPunk

					Clothing
					-->

				</article>

				<!-- Misc -->
				<article style="width: 45rem;" id="misc">
					<h2 class="major">Miscellaneous</h2>
					<div>I love watching movies, indie musics and Drama, these are some of my favorite movies.</div>
					<span class="image left">Blade Runner<img src="images/BR.jpg" alt=""/></span>
					<span class="image right">Blade Runner 2049<img src="images/BR2049.jpg" alt="" /></span>

					<span class="image left">The Matrix 1 2 3<img src="images/theM.jpg" alt=""/></span>
					<span class="image right">The Grand Budapest Hotel<img src="images/theGB.jpg" alt="" /></span>

					<span class="image left">Sleep No More: The Interactive and Immersive Theater<img src="images/SleepNoMore.jpeg" alt="" /></span>
					<span class="image right">National Theater Live: The Lehman Trilogy<img src="images/theLehman.jpg" alt="" /></span>

					<span class="image left">Once Upon a Time in...Hollywood<img src="images/Hollywood.jpeg" alt="" /></span>
					<span class="image right">Fight Club<img src="images/fightclub.png" alt="" /></span>


					<span class="image left">The Death of Stalin<img src="images/theDeathofStalin.jpg" alt="" /></span>
					<span class="image right">The Irishman<img src="images/TheIrishman.jpg" alt="" /></span>

					<span class="image right">La La Land<img src="images/LA.jpg" alt="" /></span>


				</article>

			</div>

			<!-- Footer -->
			<footer id="footer">
				<p class="copyright">&copy; nealtsai.</p>
			</footer>

		</div>

		<!-- BG -->
		<div id="bg"></div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
		<script src="assets/js/responsive_waterfall.js"></script>


	</body>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
</html>
