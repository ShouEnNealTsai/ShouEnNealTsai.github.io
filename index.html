<!DOCTYPE HTML>
<html>
	<head>
		<title>Shou En Tsai</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
			<header id="header">
				<!--
				<div class="logo">
					<span class="icon fa-code"></span>
				</div>
			-->
				<div class="content">
					<div class="inner">
						<h1>Shou-En Tsai</h1>
						<p>HCI Researcher
							<br/>
							Physiological Computing
							<br/>
							Human Computer Integration(HInt)
							<br/>
							Actively Applying for HCI Ph.D.
						</p>

					</div>
				</div>
				<nav>
					<ul>
						<li><a href="#about">About</a></li>
						<li><a href="#research">Research</a></li>
						<li><a href="#projects">Projects</a></li>
						<li><a href="#misc">Misc</a></li>
					</ul>
				</nav>
			</header>

			<!-- Main -->
			<div id="main">

				<!-- About -->
				<article style="width: 55rem;" id="about">
					<h2 class="major">About</h2>
					<span class="image bg" style="display:block; margin:auto;"><img src="images/me.jpg" alt="" /></span>
					<p>
						<br/>
						I am an HCI researcher who want to pursue a Ph.D. I am interested in tangible user interface on body and human-computer integration (HInt) devices for physical and cognitive enhancement. My research interest is to break the boundary between the human body and devices, to meld users and tech objects being used. The tangible interfaces experiences apply to coupling users and systems, a medium to carry ambiguous interfaces instantiated among humans, devices, the physical world, and cyberspace.
						<br/>
						<br/>
						I am currently working with Prof. <a href="http://mislab.cs.nthu.edu.tw/" target="_blank">Min-Chun Hu</a> in NTHU, developing a portable device about the immersive experience of olfactory feedback in VR. I'm also working with Prof. <a href="https://www.cs.nycu.edu.tw/members/detail/cswei?locale=en" target="_blank">Chun-Shu Wei</a>, a project about moving stimuli and the modulation of peripersonal space in Virtual Reality.
					</p>

					<h3 class="major">Education</h3>

					<div style="font-weight: bold;">National Tsing-Hua University<span style="font-weight: normal;">, Hsinchu, Taiwan</span><span style="float:right;">09/2015 - 06/2020</span></div>
					<ul style="padding-left: 5%;">
						<li>B.Sc. Electrical Engineering and Computer Science</li>
					</ul>


					<h3 class="major">Others</h3>
					<ul class="icons">
						<li><a href="https://twitter.com/leo12330731" target="_blank" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://linkedin.com/in/shou-en-neal-tsai-860812" target="_blank" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="mailto:leo12330731@gmail.com" class="icon fa-envelope"><span class="label">EMail</span></a></li>
					</ul>

				</article>

				<!-- Experiences -->
				<article id="research">
					<h2 class="major" id="researchHead">Research</h2>

					<section class="time-line-box" style="margin-bottom:120px;">
					 <div class="swiper-container text-center">
							 <div class="swiper-wrapper">
									 <div class="swiper-slide">
										 <a href="#UCSD" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">07/2017 - 08/2017</span></div>
										 <div class="status"><span>UCSD ProtoLab Visiting Student</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#GREENS" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">07/2019 - 10/2019</span></div>
										 <div class="status"><span>GREENS</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#olfactory" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">07/2019 - Present</span></div>
										 <div class="status"><span>Virtual Visual Cues Influence the Olfactory Perception</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#PPSVR" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">06/2021 - Present</span></div>
										 <div class="status"><span>Moving Stimuli and the Modulation of Peripersonal Space in VR</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#designfiction" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">07/2020 - Present</span></div>
										 <div class="status"><span>Design Fiction: Dream Interface Game Console</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#riprap" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">07/2020 - Present</span></div>
										 <div class="status"><span>Collaborative and Tangible VR Riprap Tutorial for Children</span></div>
										 </a>
									 </div>
							 </div>
							 <div class="swiper-pagination"></div>
					 </div>
			</section>

					<h3 class="major" id="UCSD">UCSD ProtoLab</h3>
					<div style="font-weight: bold;">Visiting Student, ProtoLab, UC San Diego<span style="font-weight: normal;"> - advised by Prof. <a href="http://spdow.ucsd.edu/" target="_blank">Steven Dow</a></span>
						<span style="float: right;">07/2017 - 08/2017</span></div>

						<ul style="padding-left: 5%;">
							<li>
								 Assist visiting scholar, <a href="https://scholar.google.dk/citations?user=yiqsgWcAAAAJ&hl=en" target="_blank">Nanna Inie</a> to develop the software that her research projects needed.
							</li>
							<br/>
							<br/>
						</ul>
						<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
							<a href="#researchHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
						</div>

					<h3 class="major" id="GREENS">GREENS</h3>
					<span class="image half"><img src="images/GREENS_ONE.png" alt="" /></span>
					<span class="image half" style="margin-bottom:20px;"><img src="images/GREENS_TWO.png" alt="" /></span>
					<div style="font-weight: bold;">Research Assistant, IDD Lab, NCTU<span style="font-weight: normal;">- advised by Prof. <a href="https://twitter.com/dayuanhuang" target="_blank">Da-Yuan Huang</a></span>
						<span style="float: right;">07/2019 - 10/2019</span></div>
						<ul style="padding-left: 5%;">
							<li>
								 GREENS: Revealing invisible environment data as an auto-generated melody with human collaboration
								 <br/>
								 GREENS was a system that would auto-generated music from invisible environmental data and humans' status. GREENS provide a fluid experience by serial voice inputs, entailing the outputs of jazzy AI music, sci-fi projecting images, and a mechanical ecosystem model. I used Google Dialogflow to build the voice-based interface. Users' voice inputs enable the Coral TPU board to trigger the pygame module to play the auto-generated jazz music produced by Tensorflow and pymidi module, and also would change our projecting images controlled by Madmapper.
							</li>
							<br/>
							<li>
								 GREENS won the Jury's Choice Award Honorable Mention and People's Choice Award.
							</li>
							<br/>
							<br/>
						</ul>

						<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
							<a href="#researchHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
						</div>


					<h3 class="major" id="olfactory">Virtual Visual Cues Influence the Olfactory Perception</h3>
					<span class="image left" style="margin-bottom:40px;"><img style=" height: 400px; object-fit: fill;" src="images/Olfactory_Display.png" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 400px; object-fit: cover;" src="images/Olfactory_Display_2.png" alt="" /></span>
					<div style="font-weight: bold;">Research Assistant, MISLab, NTHU
						<span style="font-weight: normal;"> - advised by Prof. 	<a href="http://mislab.cs.nthu.edu.tw/" target="_blank">Min-Chun Hu</a>
						</span>
						<span style="float: right;">07/2019 - Present</span>
					</div>
					<ul style="padding-left: 5%;">
						<li>I designed a portable olfactory display for VR and an experiment to investigate whether visual cues can fool or stimulate the VR smell experience, regarding intensity and directionality. The display employed an Arduino as a controller to change the different levels of olfactory characteristics. The olfactory display unit is composed of a scent generator and a scent deliverer. Regarding the scent generator, I put an absorbent cotton core inside a reservoir filled with essential oil to absorb the scent generator's oil. Besides, I attached an ultrasonic atomizer to the cotton core to diffuse the odor into the air. For the scent deliverer, a 5-V fan with a two-pin connector provided the active airflow delivery. I used a MOSFET to modulate the airflow intensity by pulse-width modulation (PWM). The result showed cross-modal interactions between visually virtual odor cues in VR and real olfactory stimuli from the device.</li>
						<br/>
					</ul>
					<ul style="padding-left: 5%;">
						<li>
							This research has been successfully published at IEEE VR 2021. You can read here, DOI: <span style="font-weight: normal; color:#f88d9f"><a href="https://ieeexplore.ieee.org/document/9417703" target="_blank"> 10.1109/VR50410.2021.00050 </a></span>
						</li>
					</ul>
					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#researchHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>

					<h3 class="major" id="PPSVR">Moving Stimuli and the Modulation of Peripersonal Space in VR</h3>
					<!--
					<span class="image left" style="margin-bottom:40px;"><img style="height: 350px; object-fit: fill;" src="images/Design_Fiction_Poster.png" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/Design_Fiction_2.jpeg" alt="" /></span>
					-->
					<div style="font-weight: bold;">Research Assistant, The Cognitive Engineering and Computational Neuroscience Lab, NYCU<span style="font-weight: normal;"> - advised by Prof. <a href="https://www.cs.nycu.edu.tw/members/detail/cswei?locale=en" target="_blank">Chun-Shu Wei</a></span>
						<span style="float: right;">06/2021 - Present</span></div>
					<ul style="padding-left: 5%;">
						<li>I join CECNL Lab with my side project about backward Peripersonal Space(PPS) augmented human device. After discussions with Professor Chun-Shu Wei, we pivoted the research direction to moving stimuli and the modulation of peripersonal space in a virtual reality environment. We want to investigate how the peripersonal space would be formed in virtual reality by the different motion patterns and discuss its influence on the immersion experience. Previous works have realized the original experiments in the VR environment. Also, they’ve discussed how different object appearances or environments would affect the PPS boundary. However, lack of studies discusses the object moving patterns that would only exist in VR environments would affect the PPS forming. As a result, we aim to discuss the relationship between PPS and some unreal motions such as a discrete move or random appearance which only exist in a VR environment. Our contribution is to find out that the unrealistic moving patterns would/would not make people form their peripersonal space.
						</li>
						<br/>
					</ul>
					<ul style="padding-left: 5%;">
						<li>
							The experiment has five modes, respectively, continuously approaching to the participants, continuously moving away from the participants, discretely approaching to the participants, discretely moving away from the participants, and randomly appearing at the position(, which we define as discrete random move). A continuous move means the ball’s motion is smoothly moving from the starting point to the end point with a specific speed, mimicking the real world situation. On the other hand, a discrete move represents a ball would only appear on the specific points and stay there for some time and then appear to the next position.
							<br/>
							The ball would approach/move away the participants at a constant speed and start at position 0m from the front of the participants, which is 17m. In this range of the movement, there were five equal-distance positions including at the starting point and the ending point. When the balls went through one of the five points, it would randomly activate the haptic feedback by HTC controllers. Each point is, respectively, P1 (3m), P2 (6m), P3 (9m), P4 (12m) and P5 (15m). Once the participants sensed the haptic feedback, they would push the button of the hand controllers to record the reaction time (calculating from activating the wearable device). Besides, there was only one haptic feedback in each trial. Each participant had to test 20 trials for each mode, 12 active trials and 8 inactive trials. We calculated the PPS boundary by the participants’ reaction times. We used a sigmoidal function to describe the relationship between tactile (RTs) and the timing at which haptic stimuli were delivered. The time of haptic stimulation was that experimental time P1 corresponds to 300 ms, P2 to 800 ms, P3 to 1500 ms, P4 to 2200 ms, and P5 to 2700 ms. The sigmoidal function was described by the following equation:
							<br/>
							<span class="image half" style="margin-bottom:5px; margin-top:5px;"><img src="images/HapticVision_function.png" alt="" /></span>
							<br/>
							where x represents the independent variable (i.e., the timing of cold feeling delivery in ms), y the dependent variable (i.e., the reaction time), ymin and ymax the lower and upper saturation levels of the sigmoid, xc the value of the abscissa at the central point of the sigmoid, which is the value of x at which y = (ymin+ ymax)/2 and b establishes the slope of the sigmoid at the central point. And xc would be the point of the PPS boundary.
						</li>
						<br/>
					</ul>
					<ul style="padding-left: 5%;">
						<li>
							About my Backward PPS device, <span style="font-weight: normal;"><a href="HapticVision" target="_blank"> read here</a></span>
						</li>
					</ul>
					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#researchHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>

					<h3 class="major" id="designfiction">Design Fiction: Dream Interface Game Console</h3>
					<span class="image left" style="margin-bottom:40px;"><img style="height: 330px; object-fit: fill;" src="images/Design_Fiction_Poster.png" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 330px; object-fit: cover;" src="images/Design_Fiction_2.jpeg" alt="" /></span>
					<div style="font-weight: bold;">Research Assistant, Everyday Interaction Lab, NYCU<span style="font-weight: normal;"> - advised by Prof. <a href="https://dcat.nctu.edu.tw/en/members/%e9%99%b3%e7%9b%88%e7%be%bd/#content" target="_blank">Ying-Yu Chen</a></span>
						<span style="float: right;">07/2020 - Present</span></div>
					<ul style="padding-left: 5%;">
						<li>I join Professor Ying-Yu Chen's weekly Research Through Design(RtD) study group. It is an informal seminar to analyze some classic and influential papers about RtD and discuss some research topic trends in the HCI community. This summer, the study group decided to execute our own speculative design by design fiction. My fiction is about a dream interface, a game console that controls the user's brain through decoding users' brainwaves and encoding the information by changing the electromagnetic field to create any lucid dream. </li>
						<br/>
						<br/>
					</ul>
					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#researchHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>


					<h3 class="major" id="riprap">Collaborative and Tangible VR Riprap Tutorial for Children</h3>

						<span class="image left" style="margin-bottom:40px;"><img style="height: 420px; object-fit: cover;" src="images/riprap_field.png" alt="" /></span>
						<span class="image right" style="margin-bottom:40px;"><img style="height: 420px; object-fit: fill;" src="images/riprap_code.png" alt="" /></span>

					<div style="font-weight: bold;">Research Assistant, Everyday Interaction Lab, NYCU<span style="font-weight: normal;"> - advised by Prof. <a href="https://dcat.nctu.edu.tw/en/members/%e9%99%b3%e7%9b%88%e7%be%bd/#content" target="_blank">Ying-Yu Chen</a></span>
						<span style="float: right;">07/2020 - Present</span></div>
					<ul style="padding-left: 5%;">
						<li>We want to preserve evanescent Taiwanese Hakka style riprap techniques due to urbanization by developing a collaborative and tangible VR riprap tutorial for children. We choose to build a VR multiplayer system since ripraps need many people to build together, while the tangible way in VR, such as realizing gravity, enables children to experience intuitively while breaking the spatial limits but remaining some trueness. The pictures show our field studies and our qualitative research process. </li>
					</ul>

					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#researchHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>



<!--
					<div style="font-weight: bold;">Independent Researcher<span style="font-weight: normal;">
						<span style="float: right;">08/2017 - Present</span></div>
					<ul style="padding-left: 5%;">
						<li>We want to explore and construct a system, HapticVision, to expand the backward peripersonal space with tactile feedback or other somatosensory feedback. HapticVision is a new system concept that uses a sensing device to detect backward visual stimuli. Once receiving stimuli, it will activate individuals’ wearable device to give temperature-haptic feedback to extend the Peripersonal Space boundaries.</li>
					</ul>
-->
				</article>

				<!-- Work -->
				<article id="projects">
					<h2 class="major" id="projectsHead">Projects</h2>

					<section class="time-line-box" style="margin-bottom:120px;">
					 <div class="swiper-container text-center">
							 <div class="swiper-wrapper">
									 <div class="swiper-slide">
										 <a href="#LimbCare" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">01/2017 - 12/2017</span></div>
										 <div class="status"><span>LimbCare</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#HapticVision" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">12/2017 - 11/2018</span></div>
										 <div class="status"><span>HapticVision</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#Odrive" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">03/2018 - 04/2018</span></div>
										 <div class="status"><span>Oculomotor Drive</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#PPunk" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">07/2020 - 09/2020</span></div>
										 <div class="status"><span>PhysioPunk</span></div>
										 </a>
									 </div>
									 <div class="swiper-slide">
										 <a href="#FGambler" onmouseover="this.style.color='#f88d9f';" onmouseout="this.style.color='white';">
										 <div class="timestamp"><span class="date">09/2020 - Present</span></div>
										 <div class="status"><span>Forced Gambler</span></div>
										 </a>
									 </div>


							 </div>
							 <div class="swiper-pagination"></div>
					 </div>
				 </section>

					<!--
					<div style="font-weight: bold;">President, Quarter to Ten<span style="font-weight: normal;">
						<span style="float: right;">01/2018 - 07/2018</span></div>
					<ul style="padding-left: 5%;">
						<li>Quarter to Ten is an official student startup group in National Tsing Hua University and is a sub-organization under Tsing Hua Entrepreneur Network (TEN), an official organization to assist Tsing Hua alumni entrepreneurs and provide the needed resources and help when building their enterprise.</li>
					</ul>
					-->
					<h3 class="major" id="LimbCare">LimbCare</h3>
					<span class="image left" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/LimbCare_Hardware.png" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 350px; object-fit: cover;" src="images/LimbCare_overview.png" alt="" /></span>
					<span class="image half" style="margin-bottom:20px;"><img src="images/LimbCare_App.png" alt="" /></span>
					<div style="font-weight: bold;">The Young Entrepreneurs of the Future program<span style="font-weight: normal;">
						<span style="float: right;">01/2017 - 12/2017</span></div>
					<ul style="padding-left: 5%;">
						<li>LimbCare uses temperature control technology to avoid all the troubles of rehabilitation preparation in advance, allowing users to do hot and cold compresses in precise temperatures and periods at any time and anywhere. LimbCare also integrates information and other rehabilitation methods in the app to speed up the recovery of sports injuries. We used BLE Chip (NRF8001), which combined a Bluetooth module and System-on-a-Chip, as the main processor to control a thermoelectric cooling chip, a fan to cool a processor, and a Bluetooth connection with our app.
						</li>
						<br/>
						<br/>
					</ul>
					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#projectsHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>


					<h3 class="major" id="HapticVision">HapticVision</h3>

					<span class="image left" style=" height: 50%;"><img style="height: 400px; object-fit: fill;" src="images/HapticVision_one.png" alt="" /></span>
					<span class="image right" style="height: 50%;"><img style="height: 400px; object-fit: fill;" src="images/HapticVision_two.png" alt="" /></span>

					<span class="image left" style="margin-bottom:30px; height: 50%;"><img style="height: 350px; object-fit: fill;" src="images/HapticVision_three.png" alt="" /></span>
					<span class="image right" style="margin-bottom:30px; height: 50%;"><img style="height: 350px; object-fit: fill;" src="images/HapticVision_four.png" alt="" /></span>
					<div>
						<span style="float: right;">12/2017 - 11/2018</span>
					</div>

					<ul style="padding-left: 5%;">
							<h4 class="major">Idea</h4>
							I noticed a phenomenon called "the eye in the back of the head," which means that people can sense objects or eye gazing behind them. This phenomenon relates to the peripersonal space (PPS). PPS is a near-body multi-sensory area. It enables humans to react to environmental dangers more immediately by integrating tactile, visual, and auditory stimuli. PPS is plastic, and its boundary changes by situations. PPS can be viewed as two parts: forward PPS and backward PPS. Backward PPS boundary is much closer to our body due to the lack of the eyes, though our brain integrates the whole PPS's sensory system to process the outside information. I wanted to explore and construct a system to expand the backward PPS with tactile feedback or other somatosensory feedback. HapticVision is my first step to chase my vision of augmented humans, enhancing human cognitive ability by designing a wearable computing device as the assistance and supplement of human perceptual capabilities.
							<br/>
							<br/>
							<h4 class="major">How to Realize</h4>
							There are two steps in the experiment for me to test my designed device: the first is to measure the backward PPS boundaries, and the second is to validate the HapticVision system concept. To measure the backward PPS boundary, I used the auditory-tactile interaction task method invented by Canzoneri et al. (Extending peripersonal space representation without tool-use: evidence from a combined behavioral-computational approach. Frontiers in Behavioral Neuroscience 9, Article 4 (February 2015), 14 pages.) This method utilizes the concept that different sensory stimuli would be integrated with the PPS. The participants would receive the haptic stimuli feedback and push the end button on their hands as their reaction once they felt the feedback. During the process, there were irrelevant dynamic sounds that were approaching the subject's back. The dynamic background sounds were the stimuli crossing the PPS boundary and being the interference to determine whether the brain integrates auditory and tactile stimuli. If so, the reaction times would be significantly shorter since stimuli are inside the PPS boundary. The dynamic sounds would approach the participant at a constant speed and start at 100cm from the subjects' back. It would end at 5cm from the participants. The dynamic sounds would randomly activate the haptic feedbacks before concrete delay times, and each point has different delays, respectively T1 (5cm) 300ms, T2 (28.75cm) 800ms, T3 (52.5cm) 1500ms, T4 (76.25cm) 2200ms, and T5 (100cm) 2700ms. (See Figure 1)
							<br/>
							<br/>
							Then, I calculated the backward PPS boundary by the participants’ reaction times. I used a sigmoidal function to describe the relationship between tactile (RTs) and the timing at which haptic stimuli were delivered. The following equation described the sigmoidal function:
							<br/>
							<span class="image half" style="margin-bottom:5px; margin-top:5px;"><img src="images/HapticVision_function.png" alt="" /></span>
							<br/>
							where x represents the timing of cold feeling delivery in ms, y the reaction time, ymin and ymax the lower and upper saturation levels of the sigmoid, xc the value of the abscissa at the central point of the sigmoid, which is the value of x at which y = (ymin+ ymax)/2 and b establishes the slope of the sigmoid at the central point. And, xc would be the point of the backward PPS boundary.
							<br/>
							<br/>
							In the second step, I set up a sensing device twice the distance of the boundary radius. The device would detect the entering stimuli and then activate the wearable device and give the haptic feedback. Once the participants felt the haptic feedback, they had to push the button on their hands, and the system would record the reaction time as well. This step was the same as the first step, but the differences are that it’s twice the distance and has the Actor in the second step. “The Actor” was the stimuli that would approach participants from their back and be acted by one of my friends. The sensing device would detect whether the Actor entered the boundary and triggered the wearable device to give haptic feedback. I analyzed these reaction time model if backward PPS has really happened.
							<br/>
							<br/>
							<h4 class="major">Hardware</h4>
							I used Arduino Leonardo as the central controller. Its inputs connected three modules, five infrared sensors as the ranging sensors, a start button, and an end button. While, its outputs connected an audio amplifier module that was realized by LM384, in which the gain was 200, and the output device was an eight-ohm speaker. The sound resource was an 8-bit, 8000Hz, wav file of wave sound. I used the PWM pin of the central controller to control the output magnification of the audio amplifier. I also used the GPIO pin to control the output device of haptic feedback, containing three TEC1-04904 thermoelectric cooling chips with 5 voltages and 4 amperes. The wearable device containing three thermoelectric cooling chips was on the participants’ right wrist and would give the cold feeling haptic feedback when activated. Once the participants pushed the end button, the whole system would stop counting and record the time intervals, which were the participants’ reaction times.
							<br/>
							<br/>
							<li>
							The experiment showed that HapticVision did enlarge the backward PPS twice. But the further hardware design and a more comprehensive experiment are still needed.
							</li>
							<br/>
							<br/>
					</ul>
					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#projectsHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>

					<h3 class="major" id="Odrive">Oculomotor Drive</h3>
					<iframe width="600" height="400"
						src="https://www.youtube.com/embed/Sd3prVTcsJ8" style="margin-bottom:40px; display:block;
						margin:auto;">
					</iframe>
					<br/>
					<br/>
					<div style="font-weight: bold;">Microsoft Imagine Cup, Student Developer Competitions<span style="font-weight: normal;">
						<span style="float: right;">03/2018 - 04/2018</span></div>
					<ul style="padding-left: 5%;">
						<li>Oculomotor Drive is a gaze tracking mobile game to gamify eye exercise. My friends and I transferred Openface, the open-source face recognization library developed by CMU Professor Mahadev Satyanarayanan's lab, from Python to Swift in order to develop iOS App more easily. And we applied on tracking gazing directions. When a player stared left, right, or at the center, the app would create a basketball throwing to the rim from the direction that the player gazed. This project aims to soothe the eyeballs' muscles via the exercise of different gazing directions.</li>
					</ul>
					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#projectsHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>

					<h3 class="major" id="PPunk">PhysioPunk</h3>
					<span class="image left" style="margin-bottom:40px;"><img style="height: 400px; object-fit: cover;" src="images/PhysioPunk_Game.PNG" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 400px; object-fit: cover;" src="images/PhysioPunk_Player.PNG" alt="" /></span>
					<div>
						<span style="float: right;">07/2020 - 09/2020</span>
					</div>
					<ul style="padding-left: 5%;">
						<h4 class="major">Idea</h4>
						PhysioPunk is a tower defense game project to explore the haziness between virtuality and reality by physiological computing. Players have to attach EMG and ECG sensors on their bodies to detect EMG and ECG signals from the left shoulder, right shoulder, low back, arms, and right leg. During the game, those signals would determine the speed of enemy generation. When players are nervous, and their muscles are tight, the heart rate is high, and the generation speed would slow down. The minimal speed is ten-second per enemy. On the other hand, when users felt relaxed due to the comfortable status of the game playing and their muscles lose, heart rate is low, the generation speed would accelerate, up to one second per enemy.
						<br/>
						<br/>
						<h4 class="major">System Implementation</h4>
						The game structure is based on an open-source Unity Learning Project, Tower Defense Template. I changed this open source project and connect it with Arduino to receive EMG data from Arduino. We use Arduino Uno board, AD8232 SparkFun Heart Rate Monitor, and Grove EMG Detector. To detect the user's heart rate, the AD8232 SparkFun Heart Rate Monitor has three pins to connect the dry electrodes attached to RA (Right Arm), LA (Left Arm), and RL (Right Leg). Grove EMG detector gathers small muscle signals from the attached electrodes. In MAGditate Fluid, we attach three electrodes on the left shoulder, right shoulder, and low back.
						<br/>
						<br/>
						<h4 class="major">Idea Philosophy</h4>
						I envision PhysioPunk as a positive-negative feedback system, which has two states' main directions. The positive-negative feedback system is to mimic the emotional hormone feedback system, such as Adrenaline. I used an epidermal way to instantiate the result of these hormones. More importantly, I made players feel connected with this game as a whole: they are not just immersive, but their biological data is bundling with the game's status. On the flip side, the game is also affected by the players' status. They intertwine and co-influence.
						<br/>
						<br/>

					</ul>
					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#projectsHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>

					<h3 class="major" id="FGambler">Forced Gambler</h3>

					<span class="image left" style="margin-bottom:40px;"><img style="height: 400px; object-fit: cover;" src="images/FGambler_shirt.jpg" alt="" /></span>
					<span class="image right" style="margin-bottom:40px;"><img style="height: 400px; object-fit: cover;" src="images/FGambler_pants.jpg" alt="" /></span>

					<div>
						<span style="float: right;">09/2020 - Present</span>
					</div>

					<ul style="padding-left: 5%;">
						<h4 class="major">Idea</h4>
						After the primary attempt of PhysioPunk and the Design Fiction project (See in My Research Experiences), I want to extend the idea of blurring two worlds, bits and atoms, making games directly affect and control players. Also, I want to introduce "Wearable Art" into "Wearable Tech" design. As a result, I want to design and make clothing from scratch, so does the game. The clothes would be a long-sleeve Cuban collar shirt and one suit pants. The whole clothes would attach the exoskeleton and electro muscle controlling method to the shirt lining. The game would be an open-world GTA-like game. When playing the game, the game would randomly trigger the electrode pads in the shirt. The pads would send different electrical impulses to stimulate specific muscles and trigger movements when the game sends requests. This electrical impulse tries to mimic gambling's withdrawal syndrome, forcing players to go to a casino in the game world. The project, Forced Gambler, aims to break the line between virtuality and reality. I delegate an avatar and a player to be themselves and be the counterpart simultaneously. By giving duality and changing the interactive relationship, Forced Gambler interweave the game world and the player's world. The immersive experience not only completes in the rendering cyberspace but also fulfills in the tangible real space.
						<br/>
						<br/>
					</ul>
					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#projectsHead" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>


				</article>

				<!-- Misc -->
				<article style="width: 45rem;" id="misc">
					<h2 class="major">Miscellaneous</h2>
					<div>I love watching movies, indie musics and Drama, these are some of my favorite movies.</div>
					<span class="image fleft">Blade Runner<img src="images/BR.jpg" alt=""/></span>
					<span class="image fright">Blade Runner 2049<img src="images/BR2049.jpg" alt="" /></span>

					<span class="image fleft">The Matrix 1 2 3<img src="images/theM.jpg" alt=""/></span>
					<span class="image fright">The Grand Budapest Hotel<img src="images/theGB.jpg" alt="" /></span>

					<span class="image fleft">Sleep No More: The Interactive and Immersive Theater<img src="images/SleepNoMore.jpeg" alt="" /></span>
					<span class="image fright">National Theater Live: The Lehman Trilogy<img src="images/theLehman.jpg" alt="" /></span>

					<span class="image fleft">Once Upon a Time in...Hollywood<img src="images/Hollywood.jpeg" alt="" /></span>
					<span class="image fright">Fight Club<img src="images/fightclub.png" alt="" /></span>


					<span class="image fleft">The Death of Stalin<img src="images/theDeathofStalin.jpg" alt="" /></span>
					<span class="image fright">The Irishman<img src="images/TheIrishman.jpg" alt="" /></span>

					<span class="image fright">La La Land<img src="images/LA.jpg" alt="" /></span>

					<div style= "margin-bottom:60px; font-weight:bold; color:#f88d9f">
						<a href="#misc" onmouseover="this.style.color='#1bf2ce';" onmouseout="this.style.color='#f88d9f';"> ^ Back to Top</a>
					</div>

				</article>

			</div>

			<!-- Footer -->
			<footer id="footer">
				<p class="copyright">&copy; nealtsai.</p>
			</footer>

		</div>

		<!-- BG -->
		<div id="bg"></div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
		<script src="assets/js/responsive_waterfall.js"></script>


	</body>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
</html>
